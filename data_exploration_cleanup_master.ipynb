{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Twitter API Keys\n",
    "from config import (consumer_key, \n",
    "                    consumer_secret, \n",
    "                    access_token, \n",
    "                    access_token_secret)\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "###\n",
    "\n",
    "# scan timeline and append unique screen names to our list of 500 'normal' twitter users\n",
    "\n",
    "normal_tweeters = []\n",
    "\n",
    "while len(normal_tweeters)<=500:\n",
    "    normals = api.search('today',count=100,lang='en')\n",
    "    normal_tweets = normals['statuses']\n",
    "    for status in normal_tweets:\n",
    "        if status['user']['screen_name'] not in normal_tweeters:\n",
    "            if status['user']['followers_count']<1500 and\\\n",
    "            status['user']['followers_count']>15 and\\\n",
    "            status['user']['statuses_count']>100 and\\\n",
    "            status['user']['statuses_count']<20000:\n",
    "                normal_tweeters.append(status['user']['screen_name'])\n",
    "    time.sleep(30)\n",
    "\n",
    "# convert to pandas DF, save to CSV\n",
    "normal_users_df = pd.DataFrame({\n",
    "    'Screen Name':normal_tweeters\n",
    "})\n",
    "normal_users_df.to_csv('normal_user_names.csv')\n",
    "\n",
    "###\n",
    "\n",
    "# generate a basic summary table for a sample of 100 users\n",
    "num_followers = []\n",
    "num_following = []\n",
    "statuses_count = []\n",
    "test_users = random.sample(normal_tweeters,100)\n",
    "\n",
    "for tweeter in test_users:\n",
    "    profile = api.get_user(tweeter)\n",
    "    num_followers.append(profile['followers_count'])\n",
    "    num_following.append(profile['friends_count'])\n",
    "    statuses_count.append(profile['statuses_count'])\n",
    "    \n",
    "###\n",
    "\n",
    "# convert lists into a pandas DF, save to CSV\n",
    "normal_summ = pd.DataFrame({\n",
    "    'Screen Name':test_users,\n",
    "    'Number of Followers':num_followers,\n",
    "    'Number Following':num_following,\n",
    "    'Number of Statuses':statuses_count\n",
    "})\n",
    "normal_summ.to_csv('normal_user_summary.csv')\n",
    "\n",
    "###\n",
    "\n",
    "# ------ Get data for our 'fitness users' ------\n",
    "# Target Fitness Hashtags\n",
    "target_tags = [\"#nopainnogain\", \"#cardio\", \"#cycling\", \"#fitspo\", \"#exercise\", \"#gym\", \n",
    "               \"#fitfam\", \"#fitlife\",\"#fitness\", \"#fitnessaddict\", \"#gymlife\", \"#gymrat\", \n",
    "              \"#gymtime\", \"#sweat\", \"#workout\", \"#marathon\", \"#runners\", \"#fit\"\n",
    "              \"#bodybuilding\", \"#beachbody\", \"#motivation\", \"#justdoit\", \"#TrainHard\", \"#GetFit\"]\n",
    "fitness_user_accounts = {}\n",
    "\n",
    "# \"Real Person\" Filters\n",
    "min_tweets = 100\n",
    "max_tweets = 20000\n",
    "max_followers = 1500\n",
    "min_following = 50\n",
    "lang = \"en\"\n",
    "    \n",
    "#Loop through the hashtags  \n",
    "for tag in target_tags: \n",
    "    \n",
    "    # Variable for holding the oldest tweet\n",
    "    oldest_tweet=None\n",
    "    \n",
    "    # Loop through target tags\n",
    "    for x in range(7):\n",
    "        \n",
    "        public_tweets = api.search(tag, count=100, result_type=\"recent\", max_id=oldest_tweet)\n",
    "    #   pprint(public_tweets)\n",
    "    \n",
    "        # Loop through all tweets\n",
    "        for tweet in public_tweets[\"statuses\"]:\n",
    "        \n",
    "            # Find the screen name\n",
    "            user = tweet[\"user\"][\"screen_name\"]\n",
    "        \n",
    "            # Define whether user is a 'real' person or not\n",
    "            if( \n",
    "                tweet[\"user\"][\"lang\"] == \"en\" and \n",
    "                \"gym\" not in tweet[\"user\"][\"description\"] and\n",
    "                tweet[\"user\"][\"followers_count\"] < max_followers and \n",
    "                tweet[\"user\"][\"statuses_count\"] > min_tweets and \n",
    "                tweet[\"user\"][\"statuses_count\"] < max_tweets and \n",
    "                tweet[\"user\"][\"friends_count\"] > min_following):\n",
    "            \n",
    "                    # Add screen name to user list\n",
    "                    if(user not in fitness_user_accounts):\n",
    "                        fitness_user_accounts[user] = 1\n",
    "                    \n",
    "                    # If it already exists add 1 to its count\n",
    "                    else:\n",
    "                        fitness_user_accounts[user] += 1 \n",
    "    \n",
    "       \n",
    "\n",
    "print(fitness_user_accounts)\n",
    "\n",
    "###\n",
    "# Convert user_accounts object into a series\n",
    "fitness_user_accounts_pd = pd.Series(fitness_user_accounts)\n",
    "\n",
    "fitness_user_accounts_pd.sort_values(ascending=False)\n",
    "\n",
    "# Export the list\n",
    "\n",
    "fitness_user_accounts_pd.to_csv(\"fitness_users2.csv\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Fitness User Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from config import consumer_key, consumer_secret, access_token, access_token_secret\n",
    "from pprint import pprint\n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "from pprint import pprint\n",
    "\n",
    "####\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "###\n",
    "\n",
    "# Read in merged dataset \n",
    "file = \"datasets/merged_lists_032118.csv\"\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "df.head()\n",
    "\n",
    "normal_users = df[\"normal_users\"].tolist()\n",
    "fitness_users = df[\"fitness_users\"].tolist()\n",
    "# normal_users\n",
    "# fitness_users\n",
    "\n",
    "#########\n",
    "# Set target twitter accounts \n",
    "targetnormal_users = normal_users \n",
    "targetfitness_users = fitness_users\n",
    "\n",
    "# Lists to hold user accounts, tweets, dates, & sentiments\n",
    "user_acct = []\n",
    "tweet_txt = []\n",
    "tweet_dt =[]\n",
    "# Vader lists \n",
    "compound_list = []\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "\n",
    "########\n",
    "\n",
    "# Loop through all fitness users \n",
    "for target in targetfitness_users:\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # Get tweets for one page for each user (20 tweets)\n",
    "        public_tweets = api.user_timeline(target)\n",
    "\n",
    "        # Loop through all tweets\n",
    "        for tweet in public_tweets:\n",
    "\n",
    "            # Run Vader Analysis on each tweet\n",
    "            results = analyzer.polarity_scores(tweet[\"text\"])\n",
    "            compound = results[\"compound\"]\n",
    "            pos = results[\"pos\"]\n",
    "            neu = results[\"neu\"]\n",
    "            neg = results[\"neg\"]\n",
    "\n",
    "            # Add each value to the appropriate list \n",
    "            user_acct.append(target)\n",
    "            tweet_txt.append(tweet[\"text\"])\n",
    "            tweet_dt.append(tweet[\"created_at\"])\n",
    "\n",
    "            compound_list.append(compound)\n",
    "            positive_list.append(pos)\n",
    "            negative_list.append(neg)\n",
    "            neutral_list.append(neu)\n",
    "\n",
    "    except tweepy.error.TweepError:\n",
    "        print(f'Sorry, {target} not found. Skipping.')\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweet timestamps to datetime objects\n",
    "converted_timestamps = []\n",
    "for dt in tweet_dt:\n",
    "    converted_time = datetime.strptime(dt, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    converted_timestamps.append(converted_time)\n",
    "    \n",
    "# Confirm length of list \n",
    "print(len(converted_timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for all accounts and tweets\n",
    "df_fit = pd.DataFrame({\"Account\":user_acct,\n",
    "                   \"Tweet Text\":tweet_txt,\n",
    "                   \"Date\":converted_timestamps,\n",
    "                   \"Compound\":compound_list,\n",
    "                   \"Positive\":positive_list,\n",
    "                   \"Negative\":negative_list,\n",
    "                   \"Neutral\":neutral_list\n",
    "                  })\n",
    "df_fit.head()\n",
    "# Reorder columns \n",
    "df_fit2 = df_fit[['Account', 'Date', 'Tweet Text', 'Compound', 'Positive', 'Neutral', 'Negative']]\n",
    "df_fit2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save df to csv\n",
    "df_fit2.to_csv(\"datasets/fit_tweets.csv\",index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Normal User Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from config import consumer_key, consumer_secret, access_token, access_token_secret\n",
    "from pprint import pprint\n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "from pprint import pprint\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "# Read in merged dataset \n",
    "file = \"datasets/merged_lists_032118.csv\"\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "normal_users = df[\"normal_users\"].tolist()\n",
    "fitness_users = df[\"fitness_users\"].tolist()\n",
    "# normal_users\n",
    "# fitness_users\n",
    "\n",
    "# Set target twitter accounts \n",
    "targetnormal_users = normal_users \n",
    "targetfitness_users = fitness_users\n",
    "\n",
    "# Lists to hold user accounts, tweets, dates, & sentiments\n",
    "user_acct = []\n",
    "tweet_txt = []\n",
    "tweet_dt =[]\n",
    "# Vader lists \n",
    "compound_list = []\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all normal users \n",
    "for target in targetnormal_users:\n",
    "    \n",
    "    # Loop through once (20 tweets)\n",
    "\n",
    "    # Get tweets for one page for each user (20 tweets)\n",
    "    try:\n",
    "        public_tweets = api.user_timeline(target)\n",
    "\n",
    "        # Loop through all tweets\n",
    "        for tweet in public_tweets:\n",
    "\n",
    "            # Run Vader Analysis on each tweet\n",
    "            results = analyzer.polarity_scores(tweet[\"text\"])\n",
    "            compound = results[\"compound\"]\n",
    "            pos = results[\"pos\"]\n",
    "            neu = results[\"neu\"]\n",
    "            neg = results[\"neg\"]\n",
    "\n",
    "            # Add each value to the appropriate list \n",
    "            user_acct.append(target)\n",
    "            tweet_txt.append(tweet[\"text\"])\n",
    "            tweet_dt.append(tweet[\"created_at\"])\n",
    "\n",
    "            compound_list.append(compound)\n",
    "            positive_list.append(pos)\n",
    "            negative_list.append(neg)\n",
    "            neutral_list.append(neu)\n",
    "    except tweepy.error.TweepError:\n",
    "        print(f'Sorry, {target} not found. Skipping.')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweet timestamps to datetime objects\n",
    "converted_timestamps = []\n",
    "for dt in tweet_dt:\n",
    "    converted_time = datetime.strptime(dt, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    converted_timestamps.append(converted_time)\n",
    "    \n",
    "# Confirm length of list \n",
    "print(len(converted_timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for all accounts and tweets\n",
    "df_norm = pd.DataFrame({\"Account\":user_acct,\n",
    "                   \"Tweet Text\":tweet_txt,\n",
    "                   \"Date\":converted_timestamps,\n",
    "                   \"Compound\":compound_list,\n",
    "                   \"Positive\":positive_list,\n",
    "                   \"Negative\":negative_list,\n",
    "                   \"Neutral\":neutral_list\n",
    "                  })\n",
    "df_norm.head()\n",
    "# Reorder columns \n",
    "df_norm2 = df_norm[['Account', 'Date', 'Tweet Text', 'Compound', 'Positive', 'Neutral', 'Negative']]\n",
    "df_norm2.head()\n",
    "\n",
    "#Save df to csv\n",
    "df_norm2.to_csv(\"datasets/norm_tweets1.csv\",encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
